alphabet: "Nabcdefghijklmnopqrstuvwxyz!,;.? "
dataset:
  root: DB/LJspeech
  train_part: 0.95
  name: ljspeech
  sample_rate: 22050
train:
  lr: 1.e-3
  weight_decay: 1.e-6
  batch_size: 64
  trainer: Tacotron2Trainer
  use_guided_attention: True
  use_monotonic_attention: False
  seed: 42
  train_log_period: 50
  val_log_period: 20
  trainer_args:
    max_epochs: 20
    gpus: 1
    gradient_clip_val: 10
encoder:
  in_channels: 512
  cnn_layers: 3
  kernel_size: 5
  cnn_dropout: 0.5
  rnn_layers: 1
  hidden_size: 256
decoder:
  n_mel_channels: 80
  prenet_dim: 256
  prenet_layers: 5
  attention_rnn_dim: 1024
  decoder_rnn_dim: 1024
  encoder_embedding_dim: 512
  attention_dim: 128
  attention_location_n_filters: 32
  attention_location_kernel_size: 31
  teacher_forcing_ratio: 1.
  dropout_prob: 0.5
  zoneout_prob: 0.1
  gate_thr: 0.5
postnet:
  n_mel_channels: 80
  embedding_dim: 512
  kernel_size: 5
  num_layers: 5
  dropout_prob: 0.5
vocoder: waveglow # not supported yet
